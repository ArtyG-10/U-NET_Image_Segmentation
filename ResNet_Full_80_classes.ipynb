{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc324154-4552-441a-8fa4-9b356ae79487",
   "metadata": {},
   "source": [
    "# Import Libraries and set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198f34a7-c817-4ece-8d06-37f4bfaeb0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models.segmentation as segmentation\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF \n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a38b5e0-aef8-4053-b3f4-98ec18dc142f",
   "metadata": {},
   "source": [
    "# Set seed for reproductability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eedbfab3-e22a-4f8c-b74d-3e9e43f23b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 15\n",
    "random.seed(seed_val)               \n",
    "np.random.seed(seed_val)           \n",
    "torch.manual_seed(seed_val)      \n",
    "os.environ['PYTHONHASHSEED'] = str(seed_val)\n",
    "    \n",
    "if torch.cuda.is_available():        \n",
    "    torch.cuda.manual_seed(seed_val)\n",
    "    cudnn.benchmark = True \n",
    "    cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992017f-8909-4b53-bbcd-68a5b27ace93",
   "metadata": {},
   "source": [
    "# Function to resize image and mask to a fixed size (256x256) converting to Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b6a133-5816-465f-b462-b1b72c583d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_transform(image, mask):\n",
    "    # Resize image to 256x256 \n",
    "    image = TF.resize(image, (256, 256))\n",
    "    # Convert image to Tensor [3, 256, 256]\n",
    "    image = TF.to_tensor(image)\n",
    "    # Use Nearest Neighbor for masks to preserve class IDs (0, 1, 2...)\n",
    "    mask = TF.resize(mask, (256, 256), interpolation=Image.NEAREST)\n",
    "    # Convert mask to LongTensor\n",
    "    mask = torch.as_tensor(np.array(mask), dtype=torch.long)\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef658b4-7e8f-4314-8c5d-68d926d4c9c2",
   "metadata": {},
   "source": [
    "# Define the coco dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3c92e1-df37-4f64-a3f1-0d97a071edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, ann_file, transform=None, target_classes=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # 1. Define Categories\n",
    "        # If no specific classes are requested, use ALL categories\n",
    "        if target_classes:\n",
    "            self.cat_ids = self.coco.getCatIds(catNms=target_classes)\n",
    "        else:\n",
    "            self.cat_ids = sorted(self.coco.getCatIds()) # Sort ensures consistent ordering\n",
    "            \n",
    "        # 2. Create ID Mapping (Crucial for Multi-class)\n",
    "        # We map COCO IDs (e.g., 1, 90) to contiguous Neural Net IDs (1, 80).\n",
    "        # Background is always index 0.\n",
    "        self.coco_id_to_contiguous = {coco_id: i + 1 for i, coco_id in enumerate(self.cat_ids)}\n",
    "        \n",
    "        # Store class names for visualization (Index 0 is 'background')\n",
    "        coco_cats = self.coco.loadCats(self.cat_ids)\n",
    "        self.classes = ['background'] + [c['name'] for c in coco_cats]\n",
    "\n",
    "        # 3. Filter Images\n",
    "        # Only keep images containing at least one of our target categories\n",
    "        self.ids = []\n",
    "        for cat_id in self.cat_ids:\n",
    "            self.ids.extend(self.coco.getImgIds(catIds=[cat_id]))\n",
    "        self.ids = list(set(self.ids)) # Remove duplicates\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        \n",
    "        # Load image\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Load Annotations\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id, catIds=self.cat_ids)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        \n",
    "        # Create Mask (Background = 0)\n",
    "        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n",
    "\n",
    "        for ann in anns:\n",
    "            coco_id = ann['category_id']\n",
    "            \n",
    "            # Use our mapping to get the contiguous ID (1-80)\n",
    "            if coco_id in self.coco_id_to_contiguous:\n",
    "                pixel_value = self.coco_id_to_contiguous[coco_id]\n",
    "                ann_mask = coco.annToMask(ann)\n",
    "                \n",
    "                # Overwrite pixels (Last annotation wins in overlap)\n",
    "                mask[ann_mask > 0] = pixel_value\n",
    "\n",
    "        mask = Image.fromarray(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f71756-380a-462b-9df8-dd15a5d5ab25",
   "metadata": {},
   "source": [
    "# ResNet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d4af8e0-5afc-4f49-a7d3-23dd14464200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LEARNING_RATE = 1e-4 # Lower LR is better for fine-tuning pre-trained models\n",
    "BATCH_SIZE = 4       # FCN-ResNet50 is memory hungry\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ad17a7d-418d-4fbf-beac-aad14a6f1c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Full COCO Dataset...\n",
      "loading annotations into memory...\n",
      "Done (t=16.72s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.56s)\n",
      "creating index...\n",
      "index created!\n",
      "FCN-ResNet50 model ready with 81 output classes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Full COCO Dataset...\")\n",
    "data_path = './coco/train2017' \n",
    "ann_path = './coco/annotations/instances_train2017.json'\n",
    "\n",
    "val_data_path = './coco/val2017'\n",
    "val_ann_file = './coco/annotations/instances_val2017.json'\n",
    "\n",
    "# Initialize coco train dataset \n",
    "train_dataset = COCOSegmentationDataset(\n",
    "    root_dir=data_path, \n",
    "    ann_file=ann_path, \n",
    "    transform=img_transform,\n",
    "    target_classes=None \n",
    ")\n",
    "\n",
    "# Initialize coco validation dataset\n",
    "val_dataset = COCOSegmentationDataset(\n",
    "    root_dir=val_data_path, \n",
    "    ann_file=val_ann_file, \n",
    "    transform=img_transform,\n",
    "    target_classes=None\n",
    ")\n",
    "NUM_CLASSES = len(train_dataset.classes)\n",
    "\n",
    "# 1. Load Pretrained FCN with ResNet-50 Backbone\n",
    "# weights='DEFAULT' loads the COCO-pretrained weights, which helps convergence\n",
    "model = segmentation.fcn_resnet50(weights='DEFAULT')\n",
    "\n",
    "# 2. Modify the Classifier Heads\n",
    "# We need to replace the last layer to output NUM_CLASSES (e.g., 81)\n",
    "# The model has a main classifier ('classifier') and an auxiliary one ('aux_classifier')\n",
    "\n",
    "# -- Main Head --\n",
    "# Input features of the classification layer (usually 2048 for ResNet50)\n",
    "in_features = model.classifier[4].in_channels\n",
    "model.classifier[4] = nn.Conv2d(in_features, NUM_CLASSES, kernel_size=1)\n",
    "\n",
    "# -- Auxiliary Head (Helps with gradient flow) --\n",
    "# Input features for aux head (usually 1024)\n",
    "in_features_aux = model.aux_classifier[4].in_channels\n",
    "model.aux_classifier[4] = nn.Conv2d(in_features_aux, NUM_CLASSES, kernel_size=1)\n",
    "\n",
    "# 3. Move to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer & Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "# Loaders (Re-using your existing dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"FCN-ResNet50 model ready with {NUM_CLASSES} output classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e36dd9-e378-4579-bbe3-72013a57176a",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df647e7-8000-4167-9908-7e8124f20951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FCN-ResNet50 Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████████████████████████████████████| 29317/29317 [47:57<00:00, 10.19it/s, loss=0.571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Results | Train Loss: 1.0346 | Val Loss: 0.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████████████████████████████| 29317/29317 [39:25<00:00, 12.40it/s, loss=0.767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Results | Train Loss: 0.8080 | Val Loss: 0.5964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  42%|█████████████████████▉                              | 12369/29317 [14:17<19:54, 14.19it/s, loss=0.248]"
     ]
    }
   ],
   "source": [
    "print(\"Starting FCN-ResNet50 Training...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    # Progress bar with safe update interval\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", mininterval=5.0)\n",
    "    \n",
    "    for i, (images, masks) in enumerate(loop):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Safety Patch (Clamp labels)\n",
    "        mask_max = NUM_CLASSES - 1\n",
    "        masks[masks > mask_max] = 255\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # === FORWARD PASS CHANGE ===\n",
    "        # Model returns a dict: {'out': tensor, 'aux': tensor}\n",
    "        output_dict = model(images)\n",
    "        output = output_dict['out']\n",
    "        aux_output = output_dict['aux']\n",
    "        \n",
    "        # Calculate Loss (Weighted sum of main + aux)\n",
    "        loss_main = criterion(output, masks)\n",
    "        loss_aux = criterion(aux_output, masks)\n",
    "        loss = loss_main + 0.4 * loss_aux\n",
    "        # ===========================\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for v_img, v_mask in val_loader:\n",
    "            v_img = v_img.to(device)\n",
    "            v_mask = v_mask.to(device)\n",
    "            v_mask[v_mask > mask_max] = 255\n",
    "            \n",
    "            # Val only needs 'out'\n",
    "            val_out = model(v_img)['out']\n",
    "            val_loss += criterion(val_out, v_mask).item()\n",
    "            \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Results | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Save (It will be large, ~100MB+)\n",
    "    torch.save(model.state_dict(), f'fcn_resnet50_epoch_{epoch+1}.pth')\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2daab-355f-419c-b09f-794f4e51650b",
   "metadata": {},
   "source": [
    "Initialize weights and start training loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
